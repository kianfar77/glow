/*
 * Copyright 2019 The Glow Authors
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package io.projectglow.transformers.normalizevariants

import com.google.common.annotations.VisibleForTesting
import io.projectglow.common.GlowLogging
import io.projectglow.common.VariantSchemas._
import io.projectglow.vcf.InternalRowToVariantContextConverter
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.SQLUtils.structFieldsEqualExceptNullability
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

private[projectglow] object VariantSplitter extends GlowLogging {

  /**
   * Generates a new DataFrame by splitting the variants in the input DataFrame similar to what vt decompose does
   *
   * @param variantDf
   * @return dataframe of split variants
   */
  @VisibleForTesting
  private[normalizevariants] def splitVariants(variantDf: DataFrame): DataFrame = {

    if (variantDf.schema.fieldNames.contains("attributes")) {
      // TODO: Unflattened INFO field splitting
      logger.warn(
        "The variant splitting transformer does not split unflattened INFO fields within the attributes column.")
    }

    // Update splitFromMultiAllelic column, add INFO_OLD_MULTIALLELIC column (see vt decompose)
    // and posexplode alternateAlleles column
    val dfAfterAltAlleleSplit = variantDf
      .withColumn(
        splitFromMultiAllelicField.name,
        when(size(col(alternateAllelesField.name)) > 1, lit(true)).otherwise(lit(false))
      )
      .withColumn(
        infoFieldPrefix + oldMultiallelicFieldName,
        when(
          col(splitFromMultiAllelicField.name),
          concat(
            col(contigNameField.name),
            lit(":"),
            col(startField.name) + 1,
            lit(":"),
            concat_ws("/", col(refAlleleField.name), col(alternateAllelesField.name))
          )
        ).otherwise(lit(null))
      )
      .select(
        col("*"),
        posexplode(col(alternateAllelesField.name))
          .as(Array(splitAlleleIdxFieldName, splitAllelesFieldName))
      )

    // Split INFO fields
    val dfAfterInfoSplit = splitInfoFields(dfAfterAltAlleleSplit)

    // split genotypes fields, update alternateAlleles field, and drop the columns resulting from posexplode
    splitGenotypeFields(dfAfterInfoSplit)
      .withColumn(alternateAllelesField.name, array(col(splitAllelesFieldName)))
      .drop(splitAlleleIdxFieldName, splitAllelesFieldName)
  }

  /**
   * Generates a new DataFrame by splitting the info fields, based on splitAlleleIdx
   * field generated by posexplode
   *
   * @param variantDf
   * @return dataframe with split info fields
   */
  @VisibleForTesting
  private[normalizevariants] def splitInfoFields(variantDf: DataFrame): DataFrame = {
    variantDf
      .schema
      .filter(field =>
        field.name.startsWith(infoFieldPrefix) && field.dataType.isInstanceOf[ArrayType])
      .foldLeft(
        variantDf
      )(
        (df, field) =>
          df.withColumn(
            field.name,
            when(
              col(splitFromMultiAllelicField.name) &&
              size(col(field.name)) === size(col(alternateAllelesField.name)),
              array(expr(s"${field.name}[$splitAlleleIdxFieldName]"))
            ).otherwise(col(field.name))
          )
      )
  }

  /**
   * Generates a new DataFrame by splitting the genotypes subfields, based on splitAlleleIdx
   * field generated by posexplode
   *
   * @param variantDf
   * @return dataframe with split genotype subfields
   */
  @VisibleForTesting
  private[normalizevariants] def splitGenotypeFields(variantDf: DataFrame): DataFrame = {

    val gSchema = InternalRowToVariantContextConverter.getGenotypeSchema(variantDf.schema)

    if (gSchema.isEmpty) {
      variantDf
    } else {
      // pull out genotypes subfields as new columns
      val withExtractedFields = gSchema.get
        .fields
        .foldLeft(variantDf)(
          (df, field) =>
            df.withColumn(field.name, expr(s"transform(${genotypesFieldName}, g -> g.${field.name})"))
        )
        .drop(genotypesFieldName)

      // register the udf that genotypes splitter uses
      withExtractedFields
        .sqlContext
        .udf
        .register(
          "likelihoodSplitUdf",
          (numAlleles: Int, ploidy: Int, alleleIdx: Int) =>
            refAltColexOrderIdxArray(numAlleles, ploidy, alleleIdx)
        )

      // update pulled-out genotypes columns, zip them back together as the new genotypes column,
      // and drop the pulled-out columns
      gSchema.get
        .fields
        .foldLeft(withExtractedFields)(
          (df, field) =>
            field match {
              case f
                if structFieldsEqualExceptNullability(genotypeLikelihoodsField, f) |
                  structFieldsEqualExceptNullability(phredLikelihoodsField, f) |
                  structFieldsEqualExceptNullability(posteriorProbabilitiesField, f) =>
                // update genotypes subfields that have colex order using the udf
                df.withColumn(
                  f.name,
                  when(
                    col(splitFromMultiAllelicField.name),
                    expr( // TODO: Hard code the result of udf for numAlleles = 2, 3, 4 and ploidy = 2
                      s"""transform(${f.name}, c ->
                         | filter(
                         | transform(
                         | c, (x, idx) -> if (array_contains(
                         | likelihoodSplitUdf(size(${alternateAllelesField.name}) + 1,
                         | size(${callsField.name}[0]), $splitAlleleIdxFieldName + 1), idx), x, null)),
                         | x -> !isnull(x)))""".stripMargin)
                  ).otherwise(col(f.name))
                )

              case f if structFieldsEqualExceptNullability(callsField, f) =>
                // update GT calls subfield
                df.withColumn(
                  f.name,
                  when(
                    col(splitFromMultiAllelicField.name),
                    expr(
                      s"transform(${f.name}, " +
                        s"c -> transform(c, x -> if(x == 0, x, if(x == $splitAlleleIdxFieldName + 1, 1, -1))))"
                    )
                  ).otherwise(col(f.name))
                )

              case f if f.dataType.isInstanceOf[ArrayType] =>
                // update any ArrayType field with number of elements equal to number of alt alleles
                df.withColumn(
                  f.name,
                  when(
                    col(splitFromMultiAllelicField.name),
                    expr(
                      s"transform(${f.name}, c -> if(size(c) == size(${alternateAllelesField.name}) + 1," +
                        s" array(c[0], c[$splitAlleleIdxFieldName + 1]), null))"
                    )
                  ).otherwise(col(f.name))
                )

              case _ =>
                df
            }
        )
        .withColumn(genotypesFieldName, arrays_zip(gSchema.get.fieldNames.map(col(_)): _*))
        .drop(gSchema.get.fieldNames: _*)
    }

  }

  /**
   * Given the total number of (ref and alt) alleles (numAlleles), ploidy, and the index an alt allele of interest
   * (altAlleleIdx), generates an array of indices of genotypes that only include the ref allele and/or that alt allele
   * of interest in the colex ordering of all possible genotypes. The function is general and correctly calculates
   * the index array for any given set of values for its arguments.
   *
   * Example:
   * Assume numAlleles = 3 (say A,B,C), ploidy = 2, and altAlleleIdx = 2 (i.e., C)
   * Therefore, colex ordering of all possible genotypes is: AA, AB, BB, AC, BC, CC
   * and for example refAltColexOrderIdxArray(3, 2, 2) = Array(0, 3, 5)
   *
   * @param numAlleles   : total number of alleles (ref and alt)
   * @param ploidy       : ploidy
   * @param altAlleleIdx : index of alt allele of interest
   * @return array of indices of genotypes that only include the ref allele and alt allele
   *         of interest in the colex ordering of all possible genotypes.
   */
  @VisibleForTesting
  private[normalizevariants] def refAltColexOrderIdxArray(
      numAlleles: Int,
      ploidy: Int,
      altAlleleIdx: Int): Array[Int] = {

    if (ploidy < 1) {
      throw new IllegalArgumentException("Ploidy must be at least 1.")
    }
    if (numAlleles < 2) {
      throw new IllegalArgumentException(
        "Number of alleles must be at least 2 (one REF and at least one ALT).")
    }
    if (altAlleleIdx > numAlleles - 1 || altAlleleIdx < 1) {
      throw new IllegalArgumentException(
        "Alternate allele index must be at least 1 and at most one less than number of alleles.")
    }

    (numAlleles, ploidy, altAlleleIdx) match {
      case (_, 1, idx) => Array(0, idx)

      // More common cases are hard-coded for performance.
      // Note: When this function is used as spark sql udf, the general case causes performance deterioration
      // with WholeStageCodeGen. The deterioration does not happen when WholeStageCodeGen is turned off.

      case (_, 2, 1) => Array(0, 1, 2)
      case (_, 2, 2) => Array(0, 3, 5)
      case (_, 2, 3) => Array(0, 6, 9)

      case (_, 3, 1) => Array(0, 1, 2, 3)
      case (_, 3, 2) => Array(0, 4, 7, 9)
      case (_, 3, 3) => Array(0, 10, 16, 19)

      case (_, 4, 1) => Array(0, 1, 2, 3, 4)
      case (_, 4, 2) => Array(0, 5, 9, 12, 14)
      case (_, 4, 3) => Array(0, 15, 25, 31, 34)

      case _ =>
        val idxArray = new Array[Int](ploidy + 1)

        // generate vector of elements at positions p+1,p,...,2 on the altAlleleIdx'th diagonal of Pascal's triangle
        idxArray(0) = 0
        var i = 1
        idxArray(ploidy) = altAlleleIdx
        while (i < ploidy) {
          idxArray(ploidy - i) = idxArray(ploidy - i + 1) * (i + altAlleleIdx) / (i + 1)
          i += 1
        }

        // calculate the cumulative vector
        i = 1
        while (i <= ploidy) {
          idxArray(i) = idxArray(i) + idxArray(i - 1)
          i += 1
        }
        idxArray
    }
  }

  @VisibleForTesting
  private[normalizevariants] val splitAlleleIdxFieldName = "splitAlleleIdx"

  @VisibleForTesting
  private[normalizevariants] val splitAllelesFieldName = "splitAlleles"

  @VisibleForTesting
  private[normalizevariants] val oldMultiallelicFieldName = "OLD_MULTIALLELIC"

}
